{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db06291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a8295",
   "metadata": {},
   "source": [
    "## نورمال سازی دیتا ها مناسب شدن برای تحلیل احساسات"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e1588",
   "metadata": {},
   "source": [
    "## دیتای توویتر"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84494fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\Data.csv\")\n",
    "\n",
    "\n",
    "unique_classes = df['label'].unique()\n",
    "class_count = df['label'].nunique()\n",
    "\n",
    "print(\"کلاس‌های یکتا:\", unique_classes)\n",
    "print(\"تعداد کلاس‌ها:\", class_count)\n",
    "\n",
    "# ذخیره نسخه تمیزشده\n",
    "df.to_csv(\"Data_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df6d845",
   "metadata": {},
   "source": [
    "## یکه کردن کلاس لیبل "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5fd799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\Data_clean.csv\", sep=None, engine=\"python\", encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "label_col = df.columns[-1]\n",
    "\n",
    "\n",
    "df[label_col] = (\n",
    "    df[label_col]\n",
    "    .astype(str).str.strip().str.lower()\n",
    "    .map(mapping)\n",
    ")\n",
    "\n",
    "\n",
    "unmapped = df[label_col].isna()\n",
    "if unmapped.any():\n",
    "    print(\"Warning: Unknown labels found at rows:\", df[unmapped].index.tolist())\n",
    "\n",
    "\n",
    "df.to_csv(\"Data.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Labels converted to: happy, sad, neutral (saved in Data.csv).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb3d75",
   "metadata": {},
   "source": [
    "## اپر کیس کردن کلاس لیبل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fffc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\Data.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df[\"label\"] = df[\"label\"].str.upper()\n",
    "\n",
    "df.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea05e2a",
   "metadata": {},
   "source": [
    "## اسنپ فود"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d5212",
   "metadata": {},
   "source": [
    "## بک اپ گیری از دیتا در پوشه دیتا های اصلی و شمارش کلاس ها "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "file_path = Path(\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\اسنپ فود\\Snappfood - Sentiment AnalysisُSnappfoodData.csv\")\n",
    "backup_path = Path(os.getcwd()) / \"snappfood.csv\"\n",
    "shutil.copy(file_path, backup_path)\n",
    "\n",
    "detected_sep = None\n",
    "with open(file_path, 'r', encoding='utf-8-sig', errors='replace') as f:\n",
    "    sample = ''.join([next(f) for _ in range(200)])\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(sample, delimiters=[',',';','\\t','|'])\n",
    "        detected_sep = dialect.delimiter\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "candidates = [s for s in [detected_sep, ',', ';', '\\t', '|'] if s]\n",
    "df = None\n",
    "for sep in candidates:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=sep, engine='python', encoding='utf-8-sig', quoting=csv.QUOTE_MINIMAL)\n",
    "        break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if df is not None and \"label\" in df.columns:\n",
    "    print(df[\"label\"].unique())\n",
    "    print(df[\"label\"].nunique())\n",
    "    print(df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58feb2b",
   "metadata": {},
   "source": [
    "## حدف کلاس ها و  ستون هایی که به درد نمیخورن "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1941c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\snappfood.csv\"\n",
    "\n",
    "# Read as tab-separated\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# Drop label_id if it exists\n",
    "if \"label_id\" in df.columns:\n",
    "    df.drop(columns=[\"label_id\"], inplace=True)\n",
    "\n",
    "# Save back to same file\n",
    "df.to_csv(file_path, sep=\"\\t\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564233c6",
   "metadata": {},
   "source": [
    "## شمارش کلاس"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\snappfood.csv\"\n",
    "\n",
    "def load_tsv_robust(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path, sep=\"\\t\", engine=\"python\", encoding=\"utf-8-sig\",\n",
    "                           quoting=csv.QUOTE_MINIMAL)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(path, sep=\"\\t\", engine=\"python\", encoding=\"utf-8-sig\",\n",
    "                               quoting=csv.QUOTE_NONE, escapechar=\"\\\\\")\n",
    "        except Exception:\n",
    "            with open(path, \"r\", encoding=\"utf-8-sig\", errors=\"replace\") as f:\n",
    "                header = f.readline().rstrip(\"\\r\\n\")\n",
    "                cols = header.split(\"\\t\")\n",
    "                exp = len(cols)\n",
    "                rows = []\n",
    "                for line in f:\n",
    "                    parts = line.rstrip(\"\\r\\n\").split(\"\\t\", maxsplit=exp-1)\n",
    "                    if len(parts) < exp:\n",
    "                        parts += [\"\"] * (exp - len(parts))\n",
    "                    rows.append(parts)\n",
    "            df = pd.DataFrame(rows, columns=[c.replace(\"\\ufeff\", \"\").strip() for c in cols])\n",
    "            return df\n",
    "\n",
    "df = load_tsv_robust(file_path)\n",
    "\n",
    "# normalize column names just in case\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "if \"label\" in df.columns:\n",
    "    print(\"Unique classes:\", df[\"label\"].unique())\n",
    "    print(\"Number of classes:\", df[\"label\"].nunique())\n",
    "    print(\"\\nCount per class:\")\n",
    "    print(df[\"label\"].value_counts())\n",
    "else:\n",
    "    print(\"No 'label' column found. Columns are:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00952e7e",
   "metadata": {},
   "source": [
    "## تیدبل به کلاس یکه"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a403fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\snappfood.csv\"\n",
    "\n",
    "# Load as tab-separated\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Remove rows where label is exactly \"0\" or \"1\"\n",
    "df = df[~df[\"label\"].isin([\"0\", \"1\"])]\n",
    "\n",
    "# Clean the label column\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: re.sub(r'\\t.*', '', str(x)).strip())\n",
    "\n",
    "# Save back to same file\n",
    "df.to_csv(file_path, sep=\"\\t\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Check the unique classes after cleaning\n",
    "print(df[\"label\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf817fe",
   "metadata": {},
   "source": [
    "## حذف کردن رکورد هایی که nan هستند"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c70e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\snappfood.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Remove rows where label is exactly \"0\" or \"1\"\n",
    "df = df[~df[\"label\"].isin([\"0\", \"1\"])]\n",
    "\n",
    "# Clean up tabs/quotes\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: re.sub(r'\\t.*', '', str(x)).strip())\n",
    "\n",
    "# Convert \"nan\" string to real NaN\n",
    "df[\"label\"] = df[\"label\"].replace(\"nan\", np.nan)\n",
    "\n",
    "# Drop rows where label is NaN\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "\n",
    "# Save back to same file\n",
    "df.to_csv(file_path, sep=\"\\t\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(df[\"label\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799c555",
   "metadata": {},
   "source": [
    "## دیتا ست فارسی "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4cc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# مسیر فایل CSV\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\IrainianDataSet.csv\"\n",
    "\n",
    "# خواندن فایل\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# پیدا کردن ستون لیبل\n",
    "label_col = None\n",
    "for col in df.columns:\n",
    "    if col.lower() in [\"label\", \"labels\", \"emotion\", \"emotions\"]:\n",
    "        label_col = col\n",
    "        break\n",
    "\n",
    "if label_col:\n",
    "    # شمارش تعداد هر کلاس\n",
    "    class_counts = df[label_col].value_counts()\n",
    "    print(\"تعداد هر کلاس:\")\n",
    "    print(class_counts)\n",
    "else:\n",
    "    print(\"ستون لیبل در این دیتاست پیدا نشد.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b79e6",
   "metadata": {},
   "source": [
    "## دستا ست ایرانی دوم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths ---\n",
    "src = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\newdataset.csv\"\n",
    "out_main = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\combined_fa_named.csv\"\n",
    "out_backup = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\IranianDataSet2.csv\"\n",
    "\n",
    "# Make sure the source exists\n",
    "if not Path(src).exists():\n",
    "    raise FileNotFoundError(f\"Source file not found: {src}\")\n",
    "\n",
    "# Read only first two columns and name them directly\n",
    "df = pd.read_csv(\n",
    "    src,\n",
    "    header=None,          # treat file as no header row\n",
    "    usecols=[0, 1],       # keep only col 0 and 1\n",
    "    names=[\"comments\", \"label\"],\n",
    "    dtype=str,            # keep everything as string (safer for text)\n",
    "    encoding=\"utf-8\"      # adjust if your file uses another encoding\n",
    ")\n",
    "\n",
    "# Optional cleaning (safe to keep)\n",
    "df[\"comments\"] = df[\"comments\"].astype(str).str.strip()\n",
    "df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
    "\n",
    "# Save outputs\n",
    "df.to_csv(out_main, index=False, encoding=\"utf-8-sig\")\n",
    "df.to_csv(out_backup, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Saved: {out_main}\")\n",
    "print(f\"Backup: {out_backup}\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Head:\\n\", df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64faf6fa",
   "metadata": {},
   "source": [
    "## شمارش کلاس و حدف در صورت نیاز"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\IranianDataSet2.csv\")\n",
    "result = df[\"label\"].value_counts()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e26c96",
   "metadata": {},
   "source": [
    "## حذف ستون های خالی و به درد نخور"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8538a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\IranianDataSet2.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "\n",
    "df = df[df[\"label\"].str.lower() != \"other\"]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.to_csv(file_path, index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76ca1e",
   "metadata": {},
   "source": [
    "## گرفتن شمارش کلاس دوباره"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e566ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\IranianDataSet2.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "class_counts = df[\"label\"].value_counts()\n",
    "\n",
    "num_classes = class_counts.shape[0]\n",
    "\n",
    "print(\"📊 Class counts after cleaning:\")\n",
    "print(class_counts)\n",
    "print(f\"\\n🔢 Number of unique classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30427ee8",
   "metadata": {},
   "source": [
    "##  دیتا ست ایرانی سوم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43526f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\iraniandataset3.csv\")\n",
    "result = df[\"label\"].value_counts()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3984d",
   "metadata": {},
   "source": [
    "## حذف دیتا ها OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719beda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\IranianDataSet3.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "\n",
    "df = df[df[\"label\"].str.lower() != \"other\"]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.to_csv(file_path, index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db437b",
   "metadata": {},
   "source": [
    "## شمارش دوباره"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05db0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\MainData\\iraniandataset3.csv\")\n",
    "result = df[\"label\"].value_counts()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a424e52",
   "metadata": {},
   "source": [
    "## 📦  دیجی‌کالا: خوشه‌بندی داده‌ها\n",
    "\n",
    "به دلیل **نداشتن برچسب** در داده‌ها، از روش **کلاسترینگ** برای خوشه‌بندی استفاده می‌کنیم.  \n",
    "پس از انجام خوشه‌بندی، به هر خوشه یک **برچسب مناسب** اختصاص داده می‌شود.  \n",
    "\n",
    "---\n",
    "\n",
    "### 📌 الگوریتم مورد استفاده:\n",
    "- **K-Means Clustering**\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 حجم داده‌ها:\n",
    "- **۷ میلیون رکورد**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 مراحل کلی کار:\n",
    "1. **پیش‌پردازش داده‌ها** (پاک‌سازی و آماده‌سازی)\n",
    "2. **اجرای الگوریتم K-Means**\n",
    "3. **تحلیل خوشه‌ها**\n",
    "4. **برچسب‌گذاری خوشه‌ها** (Labeling)\n",
    "5. **ذخیره‌سازی نتایج**\n",
    "\n",
    "---\n",
    "\n",
    "💡 نکته: به دلیل حجم بالای داده‌ها، استفاده از **پردازش موازی** و **بهینه‌سازی حافظه** توصیه می‌شود.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca06517",
   "metadata": {},
   "source": [
    "## 🔹 مرحله ۱: حذف ستون‌های اضافه\n",
    "\n",
    "قبل از اجرای الگوریتم **K-Means**، باید فقط ستون‌های مورد نیاز برای خوشه‌بندی را نگه داریم  \n",
    "و بقیه ستون‌ها (که در فرآیند خوشه‌بندی استفاده نمی‌شوند) را حذف کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# مسیر فایل\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\digikala-comments.csv\"\n",
    "\n",
    "# خواندن CSV\n",
    "df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ستون‌هایی که می‌خواهیم نگه داریم\n",
    "keep_cols = [\"body\", \"title\", \"advantages\", \"disadvantages\", \"rate\", \"recommendation_status\"]\n",
    "\n",
    "# حذف ستون‌های اضافی\n",
    "df = df[keep_cols]\n",
    "\n",
    "# ذخیره روی همان فایل (بازنویسی)\n",
    "df.to_csv(file_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"✅ فایل تمیز شد و فقط ستون‌های اصلی باقی ماندند.\")\n",
    "print(\"ستون‌های فعلی:\", df.columns.tolist())\n",
    "print(\"تعداد ردیف‌ها:\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a5855",
   "metadata": {},
   "source": [
    "## 🔹 مرحله ۲: نرمال‌سازی و سبک‌سازی داده‌ها\n",
    "\n",
    "بعد از حذف ستون‌های غیرضروری، باید داده‌ها را **نرمال‌سازی** کنیم تا همه ویژگی‌ها در یک بازه‌ی یکسان قرار بگیرند.  \n",
    "همچنین برای کاهش مصرف حافظه و سرعت‌بخشیدن به اجرای الگوریتم K-Means، **نوع داده‌ها (Data Types)** را سبک‌سازی می‌کنیم.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95370ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- مسیرها ----------\n",
    "SRC = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\digikala-comments.csv\"\n",
    "DST = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\digikala-comments-clean-light.csv\"\n",
    "\n",
    "# ---------- تنظیمات سبک‌سازی ----------\n",
    "CHUNK = 200_000          # بسته به RAM تغییر بده\n",
    "ENC   = \"utf-8-sig\"\n",
    "KEEP_META = True         # اگر فقط text می‌خواهی، False کن\n",
    "\n",
    "# ---------- الگوها ----------\n",
    "URL_RE      = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "MENTION_RE  = re.compile(r\"[@#]\\w+\")\n",
    "EMOJI_RE    = re.compile(\"[\" \"\\U0001F300-\\U0001FAFF\" \"\\U00002700-\\U000027BF\" \"\\U0001F1E6-\\U0001F1FF\" \"]+\", flags=re.UNICODE)\n",
    "NON_TEXT_RE = re.compile(r\"[^\\w\\s\\u0600-\\u06FF]\", flags=re.UNICODE)\n",
    "MULTISPACE  = re.compile(r\"\\s+\")\n",
    "\n",
    "def strip_diacritics(s: str) -> str:\n",
    "    return \"\".join(ch for ch in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(ch))\n",
    "\n",
    "def norm_fa(t: str) -> str:\n",
    "    if not isinstance(t, str): return \"\"\n",
    "    t = t.strip()\n",
    "    if not t: return \"\"\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "    t = MENTION_RE.sub(\" \", t)\n",
    "    t = EMOJI_RE.sub(\" \", t)\n",
    "    t = t.replace(\"ي\",\"ی\").replace(\"ى\",\"ی\").replace(\"ك\",\"ک\").replace(\"ـ\",\" \")\n",
    "    t = strip_diacritics(t)\n",
    "    t = NON_TEXT_RE.sub(\" \", t)\n",
    "    t = t.lower()\n",
    "    t = MULTISPACE.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "# ---------- پردازش چانکی + سبک‌سازی ----------\n",
    "src = Path(SRC)\n",
    "if not src.exists():\n",
    "    raise FileNotFoundError(f\"Input not found: {src}\")\n",
    "\n",
    "Path(DST).unlink(missing_ok=True)\n",
    "first_write = True\n",
    "total_in = total_out = 0\n",
    "\n",
    "# برای کاهش حافظه: فقط ستون‌های موردنیاز را می‌خوانیم اگر حاضر باشند\n",
    "base_cols = [\"title\",\"advantages\",\"disadvantages\",\"body\",\"rate\",\"recommendation_status\"]\n",
    "read_sample = pd.read_csv(SRC, nrows=5, encoding=ENC, on_bad_lines=\"skip\")\n",
    "available = [c for c in base_cols if c in read_sample.columns]\n",
    "usecols = available if available else None  # اگر هدر متفاوت بود، همه را بخوان و بعد هندل کن\n",
    "\n",
    "for chunk in pd.read_csv(SRC, chunksize=CHUNK, encoding=ENC, on_bad_lines=\"skip\", usecols=usecols):\n",
    "    total_in += len(chunk)\n",
    "\n",
    "    # تضمین وجود ستون‌های متنی\n",
    "    for col in [\"title\",\"advantages\",\"disadvantages\",\"body\"]:\n",
    "        if col not in chunk.columns:\n",
    "            chunk[col] = \"\"\n",
    "        chunk[col] = chunk[col].astype(str).map(norm_fa)\n",
    "\n",
    "    # ساخت متن نهایی\n",
    "    chunk[\"text\"] = (chunk[\"title\"] + \" \" + chunk[\"advantages\"] + \" \" +\n",
    "                     chunk[\"disadvantages\"] + \" \" + chunk[\"body\"]).str.strip()\n",
    "\n",
    "    # نگه‌داشتن فقط ستون‌های لازم (سبک‌سازی)\n",
    "    if KEEP_META:\n",
    "        # rate را کم‌حجم کن، recommendation_status را category کن\n",
    "        if \"rate\" in chunk.columns:\n",
    "            # تلاش برای تبدیل امن به عدد کوچک\n",
    "            chunk[\"rate\"] = pd.to_numeric(chunk[\"rate\"], errors=\"coerce\").astype(\"float32\")\n",
    "            # می‌توانی اگر صحیح بود، به int8 هم تبدیل کنی:\n",
    "            # chunk[\"rate\"] = pd.to_numeric(chunk[\"rate\"], errors=\"coerce\").fillna(-1).astype(\"int8\")\n",
    "        else:\n",
    "            chunk[\"rate\"] = pd.NA\n",
    "\n",
    "        if \"recommendation_status\" in chunk.columns:\n",
    "            chunk[\"recommendation_status\"] = chunk[\"recommendation_status\"].astype(\"string\").str.strip().str.lower().astype(\"category\")\n",
    "        else:\n",
    "            chunk[\"recommendation_status\"] = pd.Categorical([])\n",
    "\n",
    "        out = chunk[[\"text\",\"rate\",\"recommendation_status\"]].copy()\n",
    "    else:\n",
    "        out = chunk[[\"text\"]].copy()\n",
    "\n",
    "    # حذف سطرهای خالی\n",
    "    out = out[out[\"text\"] != \"\"]\n",
    "    # حذف رکوردهای تکراری\n",
    "    out = out.drop_duplicates()\n",
    "\n",
    "    total_out += len(out)\n",
    "\n",
    "    out.to_csv(DST, mode=\"a\", index=False, encoding=\"utf-8-sig\", header=first_write)\n",
    "    first_write = False\n",
    "\n",
    "print(f\"✅ Done. Input rows: {total_in:,} -> Kept after cleaning: {total_out:,}\")\n",
    "print(f\"Saved to: {DST}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069965b",
   "metadata": {},
   "source": [
    "## 🔹 مرحله ۳: بردارسازی متن + کاهش‌بُعد (برای ۷ میلیون رکورد)\n",
    "\n",
    "در این مرحله متن‌ها را به بردارهای عددی تبدیل می‌کنیم (**Text Vectorization**) و سپس برای کارایی و سرعت بیشتر، بُعد ویژگی‌ها را کاهش می‌دهیم (**Dimensionality Reduction**).\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 هدف\n",
    "- تولید ویژگی‌های عددی از متن برای خوشه‌بندی با K-Means\n",
    "- کم‌کردن ابعاد برای کاهش مصرف RAM/زمان و بهبود کیفیت خوشه‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a549dad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\vec_out\\X_proj_part00000.npz | rows so far: 200,000\n",
      "Saved D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\vec_out\\X_proj_part00001.npz | rows so far: 371,746\n",
      "✅ Done. Parts saved in: D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\vec_out\n",
      "Each part has shape roughly: (chunk_size, proj_dim)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from scipy import sparse\n",
    "from pathlib import Path\n",
    "\n",
    "FILE = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\digikala-comments-clean-light.csv\"\n",
    "OUT_DIR = Path(r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\vec_out\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNK = 200_000\n",
    "n_features = 2**20   # ~1,048,576 ویژگی؛ در صورت کمبود RAM → 2**18\n",
    "proj_dim   = 256     # ابعاد نهایی بعد از کاهش بُعد\n",
    "\n",
    "hv = HashingVectorizer(\n",
    "    n_features=n_features,\n",
    "    alternate_sign=False,\n",
    "    norm=\"l2\",\n",
    "    lowercase=False,\n",
    ")\n",
    "\n",
    "# فیت SRP فقط یک بار روی یک/دو چانک اول\n",
    "srp = SparseRandomProjection(n_components=proj_dim, random_state=42)\n",
    "\n",
    "# مرحله 1: fit SRP روی یک چانک نمونه\n",
    "sample = pd.read_csv(FILE, nrows=CHUNK, encoding=\"utf-8-sig\")\n",
    "X_sample = hv.transform(sample[\"text\"].astype(str))\n",
    "srp.fit(X_sample)\n",
    "del sample, X_sample\n",
    "\n",
    "# مرحله 2: تبدیل و ذخیره‌ی چانکی\n",
    "part = 0\n",
    "total = 0\n",
    "for chunk in pd.read_csv(FILE, chunksize=CHUNK, encoding=\"utf-8-sig\"):\n",
    "    X_hash = hv.transform(chunk[\"text\"].astype(str))  # sparse\n",
    "    X_proj = srp.transform(X_hash)                    # sparse-ish matrix\n",
    "    # ذخیره هر پارت به npz\n",
    "    out_npz = OUT_DIR / f\"X_proj_part{part:05d}.npz\"\n",
    "    sparse.save_npz(out_npz, X_proj.tocsr())\n",
    "    total += len(chunk)\n",
    "    part += 1\n",
    "    print(f\"Saved {out_npz} | rows so far: {total:,}\")\n",
    "\n",
    "print(\"✅ Done. Parts saved in:\", OUT_DIR)\n",
    "print(\"Each part has shape roughly: (chunk_size, proj_dim)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7696d",
   "metadata": {},
   "source": [
    "ادغام دو دیتای ماتریسی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ff6ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved single file: D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\vec_out\\X_proj_all.npz\n",
      "Shape: (371746, 256)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "from scipy import sparse\n",
    "\n",
    "VEC_DIR = Path(r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\vec_out\")\n",
    "OUT_ONE = VEC_DIR / \"X_proj_all.npz\"\n",
    "\n",
    "parts = sorted(glob.glob(str(VEC_DIR / \"X_proj_part*.npz\")))\n",
    "if not parts:\n",
    "    raise FileNotFoundError(\"No part files found.\")\n",
    "\n",
    "# بارگذاری و ادغام (برای ابعاد ~۳۷۱k×۲۵۶ مشکلی از نظر RAM نیست)\n",
    "mats = [sparse.load_npz(p) for p in parts]\n",
    "X_all = sparse.vstack(mats, format=\"csr\")\n",
    "\n",
    "sparse.save_npz(OUT_ONE, X_all)\n",
    "print(\"✅ Saved single file:\", OUT_ONE)\n",
    "print(\"Shape:\", X_all.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8e9a0",
   "metadata": {},
   "source": [
    "ویژوالایز کردن اختیاری"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f854fe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تصاویر در این پوشه ذخیره شدند: D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\viz\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "\n",
    "# --------- مسیرها را تنظیم کن ---------\n",
    "CLEAN_CSV = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\digikala-comments-clean-light.csv\"  # شامل ستون text (و شاید rate/recommendation_status)\n",
    "VEC_NPZ   = r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\vec_out\\X_proj_all.npz\"            # ویژگی‌های کم‌بعدی (SRP)، اگر داری\n",
    "OUT_DIR   = Path(r\"D:\\code\\projects\\EmoLex(امولکس)\\Data\\دیجی کالا\\viz\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- فونت فارسی (اختیاری) ----------\n",
    "# اگر با حروف فارسی مشکل داشتی، یک فونت نصب‌شده مثل Tahoma/Vazirmatn را فعال کن:\n",
    "# import matplotlib\n",
    "# matplotlib.rcParams['font.family'] = 'Tahoma'  # یا 'Vazirmatn'\n",
    "\n",
    "# ---------- 1) خواندن داده‌ی متنی ----------\n",
    "df = pd.read_csv(CLEAN_CSV, encoding=\"utf-8-sig\")\n",
    "assert \"text\" in df.columns, \"ستون 'text' باید وجود داشته باشد.\"\n",
    "\n",
    "# ---------- 2) طول متن‌ها ----------\n",
    "df[\"text_len_chars\"] = df[\"text\"].astype(str).str.len()\n",
    "df[\"text_len_tokens\"] = df[\"text\"].astype(str).str.split().apply(len)\n",
    "\n",
    "# هیستوگرام طول کاراکتری\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(df[\"text_len_chars\"].clip(upper=2000), bins=50)  # کلیپ برای جلوگیری از دُم خیلی بلند\n",
    "plt.title(\"Histogram of text length (chars)\")\n",
    "plt.xlabel(\"chars\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"hist_text_len_chars.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# هیستوگرام طول توکنی\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(df[\"text_len_tokens\"].clip(upper=300), bins=50)\n",
    "plt.title(\"Histogram of text length (tokens)\")\n",
    "plt.xlabel(\"tokens\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"hist_text_len_tokens.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 3) توزیع rate (اگر وجود داشته باشد) ----------\n",
    "if \"rate\" in df.columns:\n",
    "    # سعی برای تبدیل به عدد\n",
    "    rate_num = pd.to_numeric(df[\"rate\"], errors=\"coerce\")\n",
    "    rate_num = rate_num.dropna()\n",
    "    if not rate_num.empty:\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.hist(rate_num, bins=20)\n",
    "        plt.title(\"Histogram of rate\")\n",
    "        plt.xlabel(\"rate\")\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / \"hist_rate.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "# ---------- 4) شمارش recommendation_status (اگر وجود داشته باشد) ----------\n",
    "if \"recommendation_status\" in df.columns:\n",
    "    counts = df[\"recommendation_status\"].astype(str).str.strip().value_counts()\n",
    "    plt.figure(figsize=(8,5))\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(\"Counts of recommendation_status\")\n",
    "    plt.xlabel(\"status\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"bar_recommendation_status.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# ---------- 5) پرکاربردترین کلمات (Top-N terms) ----------\n",
    "# توجه: چون متن‌ها قبلاً نرمال شده‌اند، CountVectorizer ساده کافی است.\n",
    "persian_stopwords = [\n",
    "    \"و\",\"به\",\"در\",\"از\",\"که\",\"این\",\"را\",\"با\",\"برای\",\"است\",\"می\",\"اما\",\"یا\",\"اگر\",\"نه\",\n",
    "    \"من\",\"تو\",\"او\",\"ما\",\"شما\",\"آنها\",\"یک\",\"هیچ\",\"هر\",\"بود\",\"شود\",\"هم\",\"همه\"\n",
    "]\n",
    "vec = CountVectorizer(max_features=50000, stop_words=persian_stopwords)\n",
    "# برای سرعت، نمونه‌گیری (اگر دیتاست خیلی بزرگ است):\n",
    "sample_n = min(200_000, len(df))\n",
    "sample_texts = df[\"text\"].astype(str).sample(sample_n, random_state=42)\n",
    "X_cnt = vec.fit_transform(sample_texts)\n",
    "term_freq = np.asarray(X_cnt.sum(axis=0)).ravel()\n",
    "vocab = np.array(vec.get_feature_names_out())\n",
    "top_k = 30\n",
    "top_idx = term_freq.argsort()[-top_k:][::-1]\n",
    "top_terms = vocab[top_idx]\n",
    "top_vals = term_freq[top_idx]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(range(top_k), top_vals)\n",
    "plt.xticks(range(top_k), top_terms, rotation=75, ha=\"right\")\n",
    "plt.title(f\"Top {top_k} frequent terms (sample size={sample_n:,})\")\n",
    "plt.xlabel(\"term\")\n",
    "plt.ylabel(\"freq\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"top_terms_bar.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 6) پراکنش دوبعدی (2D) از ویژگی‌ها ----------\n",
    "# دو راه:\n",
    "#   الف) اگر NPZ کم‌بعدی (SRP) داری: روی نمونه‌ای از آن SVD دو مؤلفه‌ای بزن.\n",
    "#   ب) اگر NPZ نداری: همین CountVectorizer بالا را با TruncatedSVD به 2 بُعد ببَر (کیفیت پایین‌تر، ولی دید می‌دهد).\n",
    "\n",
    "plot_saved = False\n",
    "if Path(VEC_NPZ).exists():\n",
    "    # لود ویژگی‌های کم‌بعدی (مثلاً 256 بُعد)\n",
    "    X_all = sparse.load_npz(VEC_NPZ)  # CSR (n_samples, 256)\n",
    "    n_samples = min(40_000, X_all.shape[0])\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx = rng.choice(X_all.shape[0], size=n_samples, replace=False)\n",
    "    X_s = X_all[idx]\n",
    "\n",
    "    svd2 = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X2 = svd2.fit_transform(X_s)  # (n_samples, 2)\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.scatter(X2[:,0], X2[:,1], s=3, alpha=0.35)\n",
    "    plt.title(f\"2D projection of SRP features (n={n_samples:,})\")\n",
    "    plt.xlabel(\"SVD-1\")\n",
    "    plt.ylabel(\"SVD-2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"svd_scatter_from_SRP.png\", dpi=150)\n",
    "    plt.close()\n",
    "    plot_saved = True\n",
    "\n",
    "if not plot_saved:\n",
    "    # fallback: روی همان X_cnt (CountVectorizer) 2D بگیر (نمونه کوچک‌تر)\n",
    "    svd2 = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X2 = svd2.fit_transform(X_cnt)  # (sample_n, 2)\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.scatter(X2[:,0], X2[:,1], s=3, alpha=0.35)\n",
    "    plt.title(f\"2D projection from CountVectorizer (n={sample_n:,})\")\n",
    "    plt.xlabel(\"SVD-1\")\n",
    "    plt.ylabel(\"SVD-2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"svd_scatter_from_Count.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(\"✅ تصاویر در این پوشه ذخیره شدند:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
