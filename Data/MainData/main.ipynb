{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db06291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a8295",
   "metadata": {},
   "source": [
    "## Ù†ÙˆØ±Ù…Ø§Ù„ Ø³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§ Ù‡Ø§ Ù…Ù†Ø§Ø³Ø¨ Ø´Ø¯Ù† Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e1588",
   "metadata": {},
   "source": [
    "## Ø¯ÛŒØªØ§ÛŒ ØªÙˆÙˆÛŒØªØ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84494fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\Data.csv\")\n",
    "\n",
    "\n",
    "unique_classes = df['label'].unique()\n",
    "class_count = df['label'].nunique()\n",
    "\n",
    "print(\"Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ ÛŒÚ©ØªØ§:\", unique_classes)\n",
    "print(\"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§:\", class_count)\n",
    "\n",
    "# Ø°Ø®ÛŒØ±Ù‡ Ù†Ø³Ø®Ù‡ ØªÙ…ÛŒØ²Ø´Ø¯Ù‡\n",
    "df.to_csv(\"Data_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df6d845",
   "metadata": {},
   "source": [
    "## ÛŒÚ©Ù‡ Ú©Ø±Ø¯Ù† Ú©Ù„Ø§Ø³ Ù„ÛŒØ¨Ù„ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5fd799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\Data_clean.csv\", sep=None, engine=\"python\", encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "label_col = df.columns[-1]\n",
    "\n",
    "\n",
    "df[label_col] = (\n",
    "    df[label_col]\n",
    "    .astype(str).str.strip().str.lower()\n",
    "    .map(mapping)\n",
    ")\n",
    "\n",
    "\n",
    "unmapped = df[label_col].isna()\n",
    "if unmapped.any():\n",
    "    print(\"Warning: Unknown labels found at rows:\", df[unmapped].index.tolist())\n",
    "\n",
    "\n",
    "df.to_csv(\"Data.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Labels converted to: happy, sad, neutral (saved in Data.csv).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb3d75",
   "metadata": {},
   "source": [
    "## Ø§Ù¾Ø± Ú©ÛŒØ³ Ú©Ø±Ø¯Ù† Ú©Ù„Ø§Ø³ Ù„ÛŒØ¨Ù„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fffc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\Data.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df[\"label\"] = df[\"label\"].str.upper()\n",
    "\n",
    "df.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea05e2a",
   "metadata": {},
   "source": [
    "## Ø§Ø³Ù†Ù¾ ÙÙˆØ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d5212",
   "metadata": {},
   "source": [
    "## Ø¨Ú© Ø§Ù¾ Ú¯ÛŒØ±ÛŒ Ø§Ø² Ø¯ÛŒØªØ§ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ø¯ÛŒØªØ§ Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ùˆ Ø´Ù…Ø§Ø±Ø´ Ú©Ù„Ø§Ø³ Ù‡Ø§ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "file_path = Path(\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø§Ø³Ù†Ù¾ ÙÙˆØ¯\\Snappfood - Sentiment AnalysisÙSnappfoodData.csv\")\n",
    "backup_path = Path(os.getcwd()) / \"snappfood.csv\"\n",
    "shutil.copy(file_path, backup_path)\n",
    "\n",
    "detected_sep = None\n",
    "with open(file_path, 'r', encoding='utf-8-sig', errors='replace') as f:\n",
    "    sample = ''.join([next(f) for _ in range(200)])\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(sample, delimiters=[',',';','\\t','|'])\n",
    "        detected_sep = dialect.delimiter\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "candidates = [s for s in [detected_sep, ',', ';', '\\t', '|'] if s]\n",
    "df = None\n",
    "for sep in candidates:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=sep, engine='python', encoding='utf-8-sig', quoting=csv.QUOTE_MINIMAL)\n",
    "        break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if df is not None and \"label\" in df.columns:\n",
    "    print(df[\"label\"].unique())\n",
    "    print(df[\"label\"].nunique())\n",
    "    print(df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58feb2b",
   "metadata": {},
   "source": [
    "## Ø­Ø¯Ù Ú©Ù„Ø§Ø³ Ù‡Ø§ Ùˆ  Ø³ØªÙˆÙ† Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ Ø¯Ø±Ø¯ Ù†Ù…ÛŒØ®ÙˆØ±Ù† "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1941c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\snappfood.csv\"\n",
    "\n",
    "# Read as tab-separated\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# Drop label_id if it exists\n",
    "if \"label_id\" in df.columns:\n",
    "    df.drop(columns=[\"label_id\"], inplace=True)\n",
    "\n",
    "# Save back to same file\n",
    "df.to_csv(file_path, sep=\"\\t\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564233c6",
   "metadata": {},
   "source": [
    "## Ø´Ù…Ø§Ø±Ø´ Ú©Ù„Ø§Ø³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\snappfood.csv\"\n",
    "\n",
    "def load_tsv_robust(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path, sep=\"\\t\", engine=\"python\", encoding=\"utf-8-sig\",\n",
    "                           quoting=csv.QUOTE_MINIMAL)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(path, sep=\"\\t\", engine=\"python\", encoding=\"utf-8-sig\",\n",
    "                               quoting=csv.QUOTE_NONE, escapechar=\"\\\\\")\n",
    "        except Exception:\n",
    "            with open(path, \"r\", encoding=\"utf-8-sig\", errors=\"replace\") as f:\n",
    "                header = f.readline().rstrip(\"\\r\\n\")\n",
    "                cols = header.split(\"\\t\")\n",
    "                exp = len(cols)\n",
    "                rows = []\n",
    "                for line in f:\n",
    "                    parts = line.rstrip(\"\\r\\n\").split(\"\\t\", maxsplit=exp-1)\n",
    "                    if len(parts) < exp:\n",
    "                        parts += [\"\"] * (exp - len(parts))\n",
    "                    rows.append(parts)\n",
    "            df = pd.DataFrame(rows, columns=[c.replace(\"\\ufeff\", \"\").strip() for c in cols])\n",
    "            return df\n",
    "\n",
    "df = load_tsv_robust(file_path)\n",
    "\n",
    "# normalize column names just in case\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "if \"label\" in df.columns:\n",
    "    print(\"Unique classes:\", df[\"label\"].unique())\n",
    "    print(\"Number of classes:\", df[\"label\"].nunique())\n",
    "    print(\"\\nCount per class:\")\n",
    "    print(df[\"label\"].value_counts())\n",
    "else:\n",
    "    print(\"No 'label' column found. Columns are:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00952e7e",
   "metadata": {},
   "source": [
    "## ØªÛŒØ¯Ø¨Ù„ Ø¨Ù‡ Ú©Ù„Ø§Ø³ ÛŒÚ©Ù‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a403fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\snappfood.csv\"\n",
    "\n",
    "# Load as tab-separated\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Remove rows where label is exactly \"0\" or \"1\"\n",
    "df = df[~df[\"label\"].isin([\"0\", \"1\"])]\n",
    "\n",
    "# Clean the label column\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: re.sub(r'\\t.*', '', str(x)).strip())\n",
    "\n",
    "# Save back to same file\n",
    "df.to_csv(file_path, sep=\"\\t\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Check the unique classes after cleaning\n",
    "print(df[\"label\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf817fe",
   "metadata": {},
   "source": [
    "## Ø­Ø°Ù Ú©Ø±Ø¯Ù† Ø±Ú©ÙˆØ±Ø¯ Ù‡Ø§ÛŒÛŒ Ú©Ù‡ nan Ù‡Ø³ØªÙ†Ø¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c70e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\snappfood.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Remove rows where label is exactly \"0\" or \"1\"\n",
    "df = df[~df[\"label\"].isin([\"0\", \"1\"])]\n",
    "\n",
    "# Clean up tabs/quotes\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: re.sub(r'\\t.*', '', str(x)).strip())\n",
    "\n",
    "# Convert \"nan\" string to real NaN\n",
    "df[\"label\"] = df[\"label\"].replace(\"nan\", np.nan)\n",
    "\n",
    "# Drop rows where label is NaN\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "\n",
    "# Save back to same file\n",
    "df.to_csv(file_path, sep=\"\\t\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(df[\"label\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799c555",
   "metadata": {},
   "source": [
    "## Ø¯ÛŒØªØ§ Ø³Øª ÙØ§Ø±Ø³ÛŒ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4cc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ CSV\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\IrainianDataSet.csv\"\n",
    "\n",
    "# Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ† Ù„ÛŒØ¨Ù„\n",
    "label_col = None\n",
    "for col in df.columns:\n",
    "    if col.lower() in [\"label\", \"labels\", \"emotion\", \"emotions\"]:\n",
    "        label_col = col\n",
    "        break\n",
    "\n",
    "if label_col:\n",
    "    # Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ Ù‡Ø± Ú©Ù„Ø§Ø³\n",
    "    class_counts = df[label_col].value_counts()\n",
    "    print(\"ØªØ¹Ø¯Ø§Ø¯ Ù‡Ø± Ú©Ù„Ø§Ø³:\")\n",
    "    print(class_counts)\n",
    "else:\n",
    "    print(\"Ø³ØªÙˆÙ† Ù„ÛŒØ¨Ù„ Ø¯Ø± Ø§ÛŒÙ† Ø¯ÛŒØªØ§Ø³Øª Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b79e6",
   "metadata": {},
   "source": [
    "## Ø¯Ø³ØªØ§ Ø³Øª Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¯ÙˆÙ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths ---\n",
    "src = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\newdataset.csv\"\n",
    "out_main = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\combined_fa_named.csv\"\n",
    "out_backup = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\IranianDataSet2.csv\"\n",
    "\n",
    "# Make sure the source exists\n",
    "if not Path(src).exists():\n",
    "    raise FileNotFoundError(f\"Source file not found: {src}\")\n",
    "\n",
    "# Read only first two columns and name them directly\n",
    "df = pd.read_csv(\n",
    "    src,\n",
    "    header=None,          # treat file as no header row\n",
    "    usecols=[0, 1],       # keep only col 0 and 1\n",
    "    names=[\"comments\", \"label\"],\n",
    "    dtype=str,            # keep everything as string (safer for text)\n",
    "    encoding=\"utf-8\"      # adjust if your file uses another encoding\n",
    ")\n",
    "\n",
    "# Optional cleaning (safe to keep)\n",
    "df[\"comments\"] = df[\"comments\"].astype(str).str.strip()\n",
    "df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
    "\n",
    "# Save outputs\n",
    "df.to_csv(out_main, index=False, encoding=\"utf-8-sig\")\n",
    "df.to_csv(out_backup, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Saved: {out_main}\")\n",
    "print(f\"Backup: {out_backup}\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Head:\\n\", df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64faf6fa",
   "metadata": {},
   "source": [
    "## Ø´Ù…Ø§Ø±Ø´ Ú©Ù„Ø§Ø³ Ùˆ Ø­Ø¯Ù Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\IranianDataSet2.csv\")\n",
    "result = df[\"label\"].value_counts()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e26c96",
   "metadata": {},
   "source": [
    "## Ø­Ø°Ù Ø³ØªÙˆÙ† Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ùˆ Ø¨Ù‡ Ø¯Ø±Ø¯ Ù†Ø®ÙˆØ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8538a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\IranianDataSet2.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "\n",
    "df = df[df[\"label\"].str.lower() != \"other\"]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.to_csv(file_path, index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76ca1e",
   "metadata": {},
   "source": [
    "## Ú¯Ø±ÙØªÙ† Ø´Ù…Ø§Ø±Ø´ Ú©Ù„Ø§Ø³ Ø¯ÙˆØ¨Ø§Ø±Ù‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e566ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\IranianDataSet2.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "class_counts = df[\"label\"].value_counts()\n",
    "\n",
    "num_classes = class_counts.shape[0]\n",
    "\n",
    "print(\"ğŸ“Š Class counts after cleaning:\")\n",
    "print(class_counts)\n",
    "print(f\"\\nğŸ”¢ Number of unique classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30427ee8",
   "metadata": {},
   "source": [
    "##  Ø¯ÛŒØªØ§ Ø³Øª Ø§ÛŒØ±Ø§Ù†ÛŒ Ø³ÙˆÙ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43526f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\iraniandataset3.csv\")\n",
    "result = df[\"label\"].value_counts()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3984d",
   "metadata": {},
   "source": [
    "## Ø­Ø°Ù Ø¯ÛŒØªØ§ Ù‡Ø§ OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719beda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\IranianDataSet3.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "\n",
    "df = df[df[\"label\"].str.lower() != \"other\"]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.to_csv(file_path, index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db437b",
   "metadata": {},
   "source": [
    "## Ø´Ù…Ø§Ø±Ø´ Ø¯ÙˆØ¨Ø§Ø±Ù‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05db0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\MainData\\iraniandataset3.csv\")\n",
    "result = df[\"label\"].value_counts()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a424e52",
   "metadata": {},
   "source": [
    "## ğŸ“¦  Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§: Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "\n",
    "Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ **Ù†Ø¯Ø§Ø´ØªÙ† Ø¨Ø±Ú†Ø³Ø¨** Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ Ø§Ø² Ø±ÙˆØ´ **Ú©Ù„Ø§Ø³ØªØ±ÛŒÙ†Ú¯** Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….  \n",
    "Ù¾Ø³ Ø§Ø² Ø§Ù†Ø¬Ø§Ù… Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒØŒ Ø¨Ù‡ Ù‡Ø± Ø®ÙˆØ´Ù‡ ÛŒÚ© **Ø¨Ø±Ú†Ø³Ø¨ Ù…Ù†Ø§Ø³Ø¨** Ø§Ø®ØªØµØ§Øµ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù…ÙˆØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡:\n",
    "- **K-Means Clustering**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:\n",
    "- **Û· Ù…ÛŒÙ„ÛŒÙˆÙ† Ø±Ú©ÙˆØ±Ø¯**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ Ù…Ø±Ø§Ø­Ù„ Ú©Ù„ÛŒ Ú©Ø§Ø±:\n",
    "1. **Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§** (Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ)\n",
    "2. **Ø§Ø¬Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… K-Means**\n",
    "3. **ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§**\n",
    "4. **Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§** (Labeling)\n",
    "5. **Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬**\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ’¡ Ù†Ú©ØªÙ‡: Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ** Ùˆ **Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡** ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca06517",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Ù…Ø±Ø­Ù„Ù‡ Û±: Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡\n",
    "\n",
    "Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø¬Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… **K-Means**ØŒ Ø¨Ø§ÛŒØ¯ ÙÙ‚Ø· Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒÙ…  \n",
    "Ùˆ Ø¨Ù‚ÛŒÙ‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ (Ú©Ù‡ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯) Ø±Ø§ Ø­Ø°Ù Ú©Ù†ÛŒÙ…."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„\n",
    "file_path = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\digikala-comments.csv\"\n",
    "\n",
    "# Ø®ÙˆØ§Ù†Ø¯Ù† CSV\n",
    "df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒÙ…\n",
    "keep_cols = [\"body\", \"title\", \"advantages\", \"disadvantages\", \"rate\", \"recommendation_status\"]\n",
    "\n",
    "# Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ\n",
    "df = df[keep_cols]\n",
    "\n",
    "# Ø°Ø®ÛŒØ±Ù‡ Ø±ÙˆÛŒ Ù‡Ù…Ø§Ù† ÙØ§ÛŒÙ„ (Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ)\n",
    "df.to_csv(file_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"âœ… ÙØ§ÛŒÙ„ ØªÙ…ÛŒØ² Ø´Ø¯ Ùˆ ÙÙ‚Ø· Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø§Ù‚ÛŒ Ù…Ø§Ù†Ø¯Ù†Ø¯.\")\n",
    "print(\"Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ÙØ¹Ù„ÛŒ:\", df.columns.tolist())\n",
    "print(\"ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§:\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a5855",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Ù…Ø±Ø­Ù„Ù‡ Û²: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø³Ø¨Ú©â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "\n",
    "Ø¨Ø¹Ø¯ Ø§Ø² Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒØŒ Ø¨Ø§ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ **Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ** Ú©Ù†ÛŒÙ… ØªØ§ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± ÛŒÚ© Ø¨Ø§Ø²Ù‡â€ŒÛŒ ÛŒÚ©Ø³Ø§Ù† Ù‚Ø±Ø§Ø± Ø¨Ú¯ÛŒØ±Ù†Ø¯.  \n",
    "Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡ Ùˆ Ø³Ø±Ø¹Øªâ€ŒØ¨Ø®Ø´ÛŒØ¯Ù† Ø¨Ù‡ Ø§Ø¬Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… K-MeansØŒ **Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Data Types)** Ø±Ø§ Ø³Ø¨Ú©â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95370ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Ù…Ø³ÛŒØ±Ù‡Ø§ ----------\n",
    "SRC = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\digikala-comments.csv\"\n",
    "DST = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\digikala-comments-clean-light.csv\"\n",
    "\n",
    "# ---------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³Ø¨Ú©â€ŒØ³Ø§Ø²ÛŒ ----------\n",
    "CHUNK = 200_000          # Ø¨Ø³ØªÙ‡ Ø¨Ù‡ RAM ØªØºÛŒÛŒØ± Ø¨Ø¯Ù‡\n",
    "ENC   = \"utf-8-sig\"\n",
    "KEEP_META = True         # Ø§Ú¯Ø± ÙÙ‚Ø· text Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØŒ False Ú©Ù†\n",
    "\n",
    "# ---------- Ø§Ù„Ú¯ÙˆÙ‡Ø§ ----------\n",
    "URL_RE      = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "MENTION_RE  = re.compile(r\"[@#]\\w+\")\n",
    "EMOJI_RE    = re.compile(\"[\" \"\\U0001F300-\\U0001FAFF\" \"\\U00002700-\\U000027BF\" \"\\U0001F1E6-\\U0001F1FF\" \"]+\", flags=re.UNICODE)\n",
    "NON_TEXT_RE = re.compile(r\"[^\\w\\s\\u0600-\\u06FF]\", flags=re.UNICODE)\n",
    "MULTISPACE  = re.compile(r\"\\s+\")\n",
    "\n",
    "def strip_diacritics(s: str) -> str:\n",
    "    return \"\".join(ch for ch in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(ch))\n",
    "\n",
    "def norm_fa(t: str) -> str:\n",
    "    if not isinstance(t, str): return \"\"\n",
    "    t = t.strip()\n",
    "    if not t: return \"\"\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "    t = MENTION_RE.sub(\" \", t)\n",
    "    t = EMOJI_RE.sub(\" \", t)\n",
    "    t = t.replace(\"ÙŠ\",\"ÛŒ\").replace(\"Ù‰\",\"ÛŒ\").replace(\"Ùƒ\",\"Ú©\").replace(\"Ù€\",\" \")\n",
    "    t = strip_diacritics(t)\n",
    "    t = NON_TEXT_RE.sub(\" \", t)\n",
    "    t = t.lower()\n",
    "    t = MULTISPACE.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "# ---------- Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú†Ø§Ù†Ú©ÛŒ + Ø³Ø¨Ú©â€ŒØ³Ø§Ø²ÛŒ ----------\n",
    "src = Path(SRC)\n",
    "if not src.exists():\n",
    "    raise FileNotFoundError(f\"Input not found: {src}\")\n",
    "\n",
    "Path(DST).unlink(missing_ok=True)\n",
    "first_write = True\n",
    "total_in = total_out = 0\n",
    "\n",
    "# Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø­Ø§ÙØ¸Ù‡: ÙÙ‚Ø· Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯Ù†ÛŒØ§Ø² Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ… Ø§Ú¯Ø± Ø­Ø§Ø¶Ø± Ø¨Ø§Ø´Ù†Ø¯\n",
    "base_cols = [\"title\",\"advantages\",\"disadvantages\",\"body\",\"rate\",\"recommendation_status\"]\n",
    "read_sample = pd.read_csv(SRC, nrows=5, encoding=ENC, on_bad_lines=\"skip\")\n",
    "available = [c for c in base_cols if c in read_sample.columns]\n",
    "usecols = available if available else None  # Ø§Ú¯Ø± Ù‡Ø¯Ø± Ù…ØªÙØ§ÙˆØª Ø¨ÙˆØ¯ØŒ Ù‡Ù…Ù‡ Ø±Ø§ Ø¨Ø®ÙˆØ§Ù† Ùˆ Ø¨Ø¹Ø¯ Ù‡Ù†Ø¯Ù„ Ú©Ù†\n",
    "\n",
    "for chunk in pd.read_csv(SRC, chunksize=CHUNK, encoding=ENC, on_bad_lines=\"skip\", usecols=usecols):\n",
    "    total_in += len(chunk)\n",
    "\n",
    "    # ØªØ¶Ù…ÛŒÙ† ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ\n",
    "    for col in [\"title\",\"advantages\",\"disadvantages\",\"body\"]:\n",
    "        if col not in chunk.columns:\n",
    "            chunk[col] = \"\"\n",
    "        chunk[col] = chunk[col].astype(str).map(norm_fa)\n",
    "\n",
    "    # Ø³Ø§Ø®Øª Ù…ØªÙ† Ù†Ù‡Ø§ÛŒÛŒ\n",
    "    chunk[\"text\"] = (chunk[\"title\"] + \" \" + chunk[\"advantages\"] + \" \" +\n",
    "                     chunk[\"disadvantages\"] + \" \" + chunk[\"body\"]).str.strip()\n",
    "\n",
    "    # Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø´ØªÙ† ÙÙ‚Ø· Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… (Ø³Ø¨Ú©â€ŒØ³Ø§Ø²ÛŒ)\n",
    "    if KEEP_META:\n",
    "        # rate Ø±Ø§ Ú©Ù…â€ŒØ­Ø¬Ù… Ú©Ù†ØŒ recommendation_status Ø±Ø§ category Ú©Ù†\n",
    "        if \"rate\" in chunk.columns:\n",
    "            # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø§Ù…Ù† Ø¨Ù‡ Ø¹Ø¯Ø¯ Ú©ÙˆÚ†Ú©\n",
    "            chunk[\"rate\"] = pd.to_numeric(chunk[\"rate\"], errors=\"coerce\").astype(\"float32\")\n",
    "            # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ Ø§Ú¯Ø± ØµØ­ÛŒØ­ Ø¨ÙˆØ¯ØŒ Ø¨Ù‡ int8 Ù‡Ù… ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒ:\n",
    "            # chunk[\"rate\"] = pd.to_numeric(chunk[\"rate\"], errors=\"coerce\").fillna(-1).astype(\"int8\")\n",
    "        else:\n",
    "            chunk[\"rate\"] = pd.NA\n",
    "\n",
    "        if \"recommendation_status\" in chunk.columns:\n",
    "            chunk[\"recommendation_status\"] = chunk[\"recommendation_status\"].astype(\"string\").str.strip().str.lower().astype(\"category\")\n",
    "        else:\n",
    "            chunk[\"recommendation_status\"] = pd.Categorical([])\n",
    "\n",
    "        out = chunk[[\"text\",\"rate\",\"recommendation_status\"]].copy()\n",
    "    else:\n",
    "        out = chunk[[\"text\"]].copy()\n",
    "\n",
    "    # Ø­Ø°Ù Ø³Ø·Ø±Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ\n",
    "    out = out[out[\"text\"] != \"\"]\n",
    "    # Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ\n",
    "    out = out.drop_duplicates()\n",
    "\n",
    "    total_out += len(out)\n",
    "\n",
    "    out.to_csv(DST, mode=\"a\", index=False, encoding=\"utf-8-sig\", header=first_write)\n",
    "    first_write = False\n",
    "\n",
    "print(f\"âœ… Done. Input rows: {total_in:,} -> Kept after cleaning: {total_out:,}\")\n",
    "print(f\"Saved to: {DST}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069965b",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Ù…Ø±Ø­Ù„Ù‡ Û³: Ø¨Ø±Ø¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ù…ØªÙ† + Ú©Ø§Ù‡Ø´â€ŒØ¨ÙØ¹Ø¯ (Ø¨Ø±Ø§ÛŒ Û· Ù…ÛŒÙ„ÛŒÙˆÙ† Ø±Ú©ÙˆØ±Ø¯)\n",
    "\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ù…ØªÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (**Text Vectorization**) Ùˆ Ø³Ù¾Ø³ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø§ÛŒÛŒ Ùˆ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±ØŒ Ø¨ÙØ¹Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… (**Dimensionality Reduction**).\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Ù‡Ø¯Ù\n",
    "- ØªÙˆÙ„ÛŒØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø§Ø² Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ K-Means\n",
    "- Ú©Ù…â€ŒÚ©Ø±Ø¯Ù† Ø§Ø¨Ø¹Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù RAM/Ø²Ù…Ø§Ù† Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØª Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a549dad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\vec_out\\X_proj_part00000.npz | rows so far: 200,000\n",
      "Saved D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\vec_out\\X_proj_part00001.npz | rows so far: 371,746\n",
      "âœ… Done. Parts saved in: D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\vec_out\n",
      "Each part has shape roughly: (chunk_size, proj_dim)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from scipy import sparse\n",
    "from pathlib import Path\n",
    "\n",
    "FILE = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\digikala-comments-clean-light.csv\"\n",
    "OUT_DIR = Path(r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\vec_out\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNK = 200_000\n",
    "n_features = 2**20   # ~1,048,576 ÙˆÛŒÚ˜Ú¯ÛŒØ› Ø¯Ø± ØµÙˆØ±Øª Ú©Ù…Ø¨ÙˆØ¯ RAM â†’ 2**18\n",
    "proj_dim   = 256     # Ø§Ø¨Ø¹Ø§Ø¯ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø¹Ø¯ Ø§Ø² Ú©Ø§Ù‡Ø´ Ø¨ÙØ¹Ø¯\n",
    "\n",
    "hv = HashingVectorizer(\n",
    "    n_features=n_features,\n",
    "    alternate_sign=False,\n",
    "    norm=\"l2\",\n",
    "    lowercase=False,\n",
    ")\n",
    "\n",
    "# ÙÛŒØª SRP ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ø±ÙˆÛŒ ÛŒÚ©/Ø¯Ùˆ Ú†Ø§Ù†Ú© Ø§ÙˆÙ„\n",
    "srp = SparseRandomProjection(n_components=proj_dim, random_state=42)\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ 1: fit SRP Ø±ÙˆÛŒ ÛŒÚ© Ú†Ø§Ù†Ú© Ù†Ù…ÙˆÙ†Ù‡\n",
    "sample = pd.read_csv(FILE, nrows=CHUNK, encoding=\"utf-8-sig\")\n",
    "X_sample = hv.transform(sample[\"text\"].astype(str))\n",
    "srp.fit(X_sample)\n",
    "del sample, X_sample\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ 2: ØªØ¨Ø¯ÛŒÙ„ Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒÛŒ Ú†Ø§Ù†Ú©ÛŒ\n",
    "part = 0\n",
    "total = 0\n",
    "for chunk in pd.read_csv(FILE, chunksize=CHUNK, encoding=\"utf-8-sig\"):\n",
    "    X_hash = hv.transform(chunk[\"text\"].astype(str))  # sparse\n",
    "    X_proj = srp.transform(X_hash)                    # sparse-ish matrix\n",
    "    # Ø°Ø®ÛŒØ±Ù‡ Ù‡Ø± Ù¾Ø§Ø±Øª Ø¨Ù‡ npz\n",
    "    out_npz = OUT_DIR / f\"X_proj_part{part:05d}.npz\"\n",
    "    sparse.save_npz(out_npz, X_proj.tocsr())\n",
    "    total += len(chunk)\n",
    "    part += 1\n",
    "    print(f\"Saved {out_npz} | rows so far: {total:,}\")\n",
    "\n",
    "print(\"âœ… Done. Parts saved in:\", OUT_DIR)\n",
    "print(\"Each part has shape roughly: (chunk_size, proj_dim)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7696d",
   "metadata": {},
   "source": [
    "Ø§Ø¯ØºØ§Ù… Ø¯Ùˆ Ø¯ÛŒØªØ§ÛŒ Ù…Ø§ØªØ±ÛŒØ³ÛŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ff6ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved single file: D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\vec_out\\X_proj_all.npz\n",
      "Shape: (371746, 256)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "from scipy import sparse\n",
    "\n",
    "VEC_DIR = Path(r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\vec_out\")\n",
    "OUT_ONE = VEC_DIR / \"X_proj_all.npz\"\n",
    "\n",
    "parts = sorted(glob.glob(str(VEC_DIR / \"X_proj_part*.npz\")))\n",
    "if not parts:\n",
    "    raise FileNotFoundError(\"No part files found.\")\n",
    "\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø§Ø¯ØºØ§Ù… (Ø¨Ø±Ø§ÛŒ Ø§Ø¨Ø¹Ø§Ø¯ ~Û³Û·Û±kÃ—Û²ÛµÛ¶ Ù…Ø´Ú©Ù„ÛŒ Ø§Ø² Ù†Ø¸Ø± RAM Ù†ÛŒØ³Øª)\n",
    "mats = [sparse.load_npz(p) for p in parts]\n",
    "X_all = sparse.vstack(mats, format=\"csr\")\n",
    "\n",
    "sparse.save_npz(OUT_ONE, X_all)\n",
    "print(\"âœ… Saved single file:\", OUT_ONE)\n",
    "print(\"Shape:\", X_all.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8e9a0",
   "metadata": {},
   "source": [
    "ÙˆÛŒÚ˜ÙˆØ§Ù„Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø§Ø®ØªÛŒØ§Ø±ÛŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f854fe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªØµØ§ÙˆÛŒØ± Ø¯Ø± Ø§ÛŒÙ† Ù¾ÙˆØ´Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯: D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\viz\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "\n",
    "# --------- Ù…Ø³ÛŒØ±Ù‡Ø§ Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù† ---------\n",
    "CLEAN_CSV = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\digikala-comments-clean-light.csv\"  # Ø´Ø§Ù…Ù„ Ø³ØªÙˆÙ† text (Ùˆ Ø´Ø§ÛŒØ¯ rate/recommendation_status)\n",
    "VEC_NPZ   = r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\vec_out\\X_proj_all.npz\"            # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù…â€ŒØ¨Ø¹Ø¯ÛŒ (SRP)ØŒ Ø§Ú¯Ø± Ø¯Ø§Ø±ÛŒ\n",
    "OUT_DIR   = Path(r\"D:\\code\\projects\\EmoLex(Ø§Ù…ÙˆÙ„Ú©Ø³)\\Data\\Ø¯ÛŒØ¬ÛŒ Ú©Ø§Ù„Ø§\\viz\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- ÙÙˆÙ†Øª ÙØ§Ø±Ø³ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ) ----------\n",
    "# Ø§Ú¯Ø± Ø¨Ø§ Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ Ù…Ø´Ú©Ù„ Ø¯Ø§Ø´ØªÛŒØŒ ÛŒÚ© ÙÙˆÙ†Øª Ù†ØµØ¨â€ŒØ´Ø¯Ù‡ Ù…Ø«Ù„ Tahoma/Vazirmatn Ø±Ø§ ÙØ¹Ø§Ù„ Ú©Ù†:\n",
    "# import matplotlib\n",
    "# matplotlib.rcParams['font.family'] = 'Tahoma'  # ÛŒØ§ 'Vazirmatn'\n",
    "\n",
    "# ---------- 1) Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÛŒ Ù…ØªÙ†ÛŒ ----------\n",
    "df = pd.read_csv(CLEAN_CSV, encoding=\"utf-8-sig\")\n",
    "assert \"text\" in df.columns, \"Ø³ØªÙˆÙ† 'text' Ø¨Ø§ÛŒØ¯ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\"\n",
    "\n",
    "# ---------- 2) Ø·ÙˆÙ„ Ù…ØªÙ†â€ŒÙ‡Ø§ ----------\n",
    "df[\"text_len_chars\"] = df[\"text\"].astype(str).str.len()\n",
    "df[\"text_len_tokens\"] = df[\"text\"].astype(str).str.split().apply(len)\n",
    "\n",
    "# Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ø·ÙˆÙ„ Ú©Ø§Ø±Ø§Ú©ØªØ±ÛŒ\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(df[\"text_len_chars\"].clip(upper=2000), bins=50)  # Ú©Ù„ÛŒÙ¾ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¯ÙÙ… Ø®ÛŒÙ„ÛŒ Ø¨Ù„Ù†Ø¯\n",
    "plt.title(\"Histogram of text length (chars)\")\n",
    "plt.xlabel(\"chars\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"hist_text_len_chars.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ø·ÙˆÙ„ ØªÙˆÚ©Ù†ÛŒ\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(df[\"text_len_tokens\"].clip(upper=300), bins=50)\n",
    "plt.title(\"Histogram of text length (tokens)\")\n",
    "plt.xlabel(\"tokens\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"hist_text_len_tokens.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 3) ØªÙˆØ²ÛŒØ¹ rate (Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯) ----------\n",
    "if \"rate\" in df.columns:\n",
    "    # Ø³Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¹Ø¯Ø¯\n",
    "    rate_num = pd.to_numeric(df[\"rate\"], errors=\"coerce\")\n",
    "    rate_num = rate_num.dropna()\n",
    "    if not rate_num.empty:\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.hist(rate_num, bins=20)\n",
    "        plt.title(\"Histogram of rate\")\n",
    "        plt.xlabel(\"rate\")\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / \"hist_rate.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "# ---------- 4) Ø´Ù…Ø§Ø±Ø´ recommendation_status (Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯) ----------\n",
    "if \"recommendation_status\" in df.columns:\n",
    "    counts = df[\"recommendation_status\"].astype(str).str.strip().value_counts()\n",
    "    plt.figure(figsize=(8,5))\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(\"Counts of recommendation_status\")\n",
    "    plt.xlabel(\"status\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"bar_recommendation_status.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# ---------- 5) Ù¾Ø±Ú©Ø§Ø±Ø¨Ø±Ø¯ØªØ±ÛŒÙ† Ú©Ù„Ù…Ø§Øª (Top-N terms) ----------\n",
    "# ØªÙˆØ¬Ù‡: Ú†ÙˆÙ† Ù…ØªÙ†â€ŒÙ‡Ø§ Ù‚Ø¨Ù„Ø§Ù‹ Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ CountVectorizer Ø³Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø§Ø³Øª.\n",
    "persian_stopwords = [\n",
    "    \"Ùˆ\",\"Ø¨Ù‡\",\"Ø¯Ø±\",\"Ø§Ø²\",\"Ú©Ù‡\",\"Ø§ÛŒÙ†\",\"Ø±Ø§\",\"Ø¨Ø§\",\"Ø¨Ø±Ø§ÛŒ\",\"Ø§Ø³Øª\",\"Ù…ÛŒ\",\"Ø§Ù…Ø§\",\"ÛŒØ§\",\"Ø§Ú¯Ø±\",\"Ù†Ù‡\",\n",
    "    \"Ù…Ù†\",\"ØªÙˆ\",\"Ø§Ùˆ\",\"Ù…Ø§\",\"Ø´Ù…Ø§\",\"Ø¢Ù†Ù‡Ø§\",\"ÛŒÚ©\",\"Ù‡ÛŒÚ†\",\"Ù‡Ø±\",\"Ø¨ÙˆØ¯\",\"Ø´ÙˆØ¯\",\"Ù‡Ù…\",\"Ù‡Ù…Ù‡\"\n",
    "]\n",
    "vec = CountVectorizer(max_features=50000, stop_words=persian_stopwords)\n",
    "# Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹ØªØŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ (Ø§Ú¯Ø± Ø¯ÛŒØªØ§Ø³Øª Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ Ø§Ø³Øª):\n",
    "sample_n = min(200_000, len(df))\n",
    "sample_texts = df[\"text\"].astype(str).sample(sample_n, random_state=42)\n",
    "X_cnt = vec.fit_transform(sample_texts)\n",
    "term_freq = np.asarray(X_cnt.sum(axis=0)).ravel()\n",
    "vocab = np.array(vec.get_feature_names_out())\n",
    "top_k = 30\n",
    "top_idx = term_freq.argsort()[-top_k:][::-1]\n",
    "top_terms = vocab[top_idx]\n",
    "top_vals = term_freq[top_idx]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(range(top_k), top_vals)\n",
    "plt.xticks(range(top_k), top_terms, rotation=75, ha=\"right\")\n",
    "plt.title(f\"Top {top_k} frequent terms (sample size={sample_n:,})\")\n",
    "plt.xlabel(\"term\")\n",
    "plt.ylabel(\"freq\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"top_terms_bar.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 6) Ù¾Ø±Ø§Ú©Ù†Ø´ Ø¯ÙˆØ¨Ø¹Ø¯ÛŒ (2D) Ø§Ø² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ----------\n",
    "# Ø¯Ùˆ Ø±Ø§Ù‡:\n",
    "#   Ø§Ù„Ù) Ø§Ú¯Ø± NPZ Ú©Ù…â€ŒØ¨Ø¹Ø¯ÛŒ (SRP) Ø¯Ø§Ø±ÛŒ: Ø±ÙˆÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¢Ù† SVD Ø¯Ùˆ Ù…Ø¤Ù„ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø²Ù†.\n",
    "#   Ø¨) Ø§Ú¯Ø± NPZ Ù†Ø¯Ø§Ø±ÛŒ: Ù‡Ù…ÛŒÙ† CountVectorizer Ø¨Ø§Ù„Ø§ Ø±Ø§ Ø¨Ø§ TruncatedSVD Ø¨Ù‡ 2 Ø¨ÙØ¹Ø¯ Ø¨Ø¨ÙØ± (Ú©ÛŒÙÛŒØª Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±ØŒ ÙˆÙ„ÛŒ Ø¯ÛŒØ¯ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯).\n",
    "\n",
    "plot_saved = False\n",
    "if Path(VEC_NPZ).exists():\n",
    "    # Ù„ÙˆØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù…â€ŒØ¨Ø¹Ø¯ÛŒ (Ù…Ø«Ù„Ø§Ù‹ 256 Ø¨ÙØ¹Ø¯)\n",
    "    X_all = sparse.load_npz(VEC_NPZ)  # CSR (n_samples, 256)\n",
    "    n_samples = min(40_000, X_all.shape[0])\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx = rng.choice(X_all.shape[0], size=n_samples, replace=False)\n",
    "    X_s = X_all[idx]\n",
    "\n",
    "    svd2 = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X2 = svd2.fit_transform(X_s)  # (n_samples, 2)\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.scatter(X2[:,0], X2[:,1], s=3, alpha=0.35)\n",
    "    plt.title(f\"2D projection of SRP features (n={n_samples:,})\")\n",
    "    plt.xlabel(\"SVD-1\")\n",
    "    plt.ylabel(\"SVD-2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"svd_scatter_from_SRP.png\", dpi=150)\n",
    "    plt.close()\n",
    "    plot_saved = True\n",
    "\n",
    "if not plot_saved:\n",
    "    # fallback: Ø±ÙˆÛŒ Ù‡Ù…Ø§Ù† X_cnt (CountVectorizer) 2D Ø¨Ú¯ÛŒØ± (Ù†Ù…ÙˆÙ†Ù‡ Ú©ÙˆÚ†Ú©â€ŒØªØ±)\n",
    "    svd2 = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X2 = svd2.fit_transform(X_cnt)  # (sample_n, 2)\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.scatter(X2[:,0], X2[:,1], s=3, alpha=0.35)\n",
    "    plt.title(f\"2D projection from CountVectorizer (n={sample_n:,})\")\n",
    "    plt.xlabel(\"SVD-1\")\n",
    "    plt.ylabel(\"SVD-2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"svd_scatter_from_Count.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(\"âœ… ØªØµØ§ÙˆÛŒØ± Ø¯Ø± Ø§ÛŒÙ† Ù¾ÙˆØ´Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
